# ğŸ§  CMU-Project: TBU

TBU (description).

---

## ğŸ“ Repository Structure

```
CMU-project/
â”œâ”€ data/
â”‚  â”œâ”€ images/                # input images (may contain subfolders like sb/, yl/)
â”‚  â””â”€ captions.jsonl         # JSONL file with image paths and reference captions
â”‚
â”œâ”€ eval/
â”‚  â”œâ”€ metrics.py             # evaluation utilities (BLEU, ROUGE, etc.)
â”‚  â””â”€ run_llama32.py         # main evaluation script
â”‚
â”œâ”€ results/
â”‚  â”œâ”€ llama32-11b/           # auto-generated results for LLaMA-3.2-Vision
â”‚  â”‚  â”œâ”€ 1021-2129.json
â”‚  â”‚  â””â”€ 1021-2149.json
â”‚  â””â”€ llava-7b/              
â”‚
â”œâ”€ scripts/
â”‚  â””â”€ run_llama32.sh         # shell script wrapper for run_llama32.py
â”‚
â”œâ”€ environment.yml           # conda environment definition
â”œâ”€ requirements.txt          # pip requirements (identical dependencies)
â””â”€ README.md                 # this documentation
```

---

## âš™ï¸ Environment Setup

### 1. Create the conda environment
```bash
conda env create -f environment.yml
conda activate unsloth_env
```


---

## ğŸ§© Input Format

### `data/captions.jsonl`
Each line represents one image and its reference captions.

```json
{"image": "yl/00001.png", "caption1": "A cat gives food to a man."}
{"image": "sb/00001.png", "caption1": "The dog takes a person for a walk."}
```

*Paths inside `"image"` must be relative to `data/images/`.*

---

## ğŸš€ Running Evaluation

### âœ… Option 1: Quick Run (recommended)
Use the shell script that activates the environment and runs evaluation automatically.

```bash
bash scripts/run_llama32.sh
bash scripts/run_llava7b.sh 
```

**What it does:**
1. Activates the conda environment `unsloth_env`
2. Calls the Python script `eval/run_llama32.py` or `eval/run_llava7b.py`
3. Saves generated captions to  
   `results/llama32-11b/<MMDD-HHMM>.json` or `results/llava-7b/<MMDD-HHMM>.json`
4. Prints selected metric scores in the terminal

Example output:
```
[OK] Generated captions saved to ../results/llama32-11b/1022-2337.json
BLEU: 0.4382
```

---

### âš¡ Option 2: Manual Python Execution
If you prefer direct control, run:
```bash
python3 eval/run_llama32.py \
  --jsonl data/captions.jsonl \
  --model_name unsloth/Llama-3.2-11B-Vision-Instruct \
  --save_dir results/llama32-11b \
  --eval_metric all
```

Available metrics:  
`bleu`, `rouge`, `cider`, `meteor`, `clip`, `all`

---

## ğŸ§¾ Output Files

Each run produces a timestamped JSON file:

```
results/llama32-11b/
â”œâ”€ 1022-2337.json      
â””â”€ 1022-2350.json     
```

Example content:
```json
[
   {
        "image_path": "../data/images/yl/00001.png",
        "generated_caption": "This image depicts a man sitting on the floor with a bowl of cat food, and a cat standing on its hind legs and holding the bowl with one paw, as if to beg for food."
    },
    {
        "image_path": "../data/images/yl/00002.png",
        "generated_caption": "The image depicts a man swimming underwater with a turtle perched on his back, showcasing a unique and intriguing scene."
    }
]
```

---

## ğŸ§® Evaluation Metrics

| Metric | Description |
|--------|--------------|
| **BLEU** | Measures n-gram precision against reference captions |
| **ROUGE-L** | Longest common subsequence overlap |
| **CIDEr** | Consensus-based metric used in COCO Captions |
| **METEOR** | Considers synonyms and word alignment |
| **CLIP Score** | Vision-language embedding similarity |

All metrics are implemented in `eval/metrics.py`.

---

## ğŸ§ª Example Workflow

```bash
# Step 1: Activate environment
conda activate unsloth_env

# Step 2: Run evaluation
bash scripts/run_llama32.sh data/captions.jsonl

# Step 3: Inspect output
cat results/llama32-11b/<timestamp>.json
```

---

## ğŸ“ˆ Result Summary Template

When reporting results, you can log metrics like this:

| Model | BLEU-4 | ROUGE-L | CIDEr | METEOR | CLIP Avg |
|-------|--------|----------|--------|---------|-----------|
| LLaMA 3.2 11B Vision | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 |

---

## ğŸ‘©â€ğŸ’» Author

*Carnegie Mellon University Ã— IITP Program*

ğŸ“§ seungbel@andrew.cmu.edu
**Seungbeen Lee**  
ğŸ“§ subeenp@andrew.cmu.edu
**Subeen Park**  
ğŸ“§ yujinle2@andrew.cmu.edu
**Yujin Lee**  
ğŸ“§ yulims@andrew.cmu.edu
**Yulim So**  

---
